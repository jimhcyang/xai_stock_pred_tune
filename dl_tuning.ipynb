{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For metrics and splitting\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout=0.0):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        # final output layer\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, input_dim)\n",
    "        return self.net(x)  # shape: (batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout=0.0):\n",
    "        super(RNN, self).__init__()\n",
    "        # We'll use simple RNN\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=n_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if n_layers>1 else 0.0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len=1, input_dim)\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim, device=x.device)\n",
    "\n",
    "        out, _ = self.rnn(x, h0) \n",
    "        # out shape: (batch_size, seq_len, hidden_dim)\n",
    "        out = out[:, -1, :]   # take last time step\n",
    "        out = self.fc(out)    # shape: (batch_size, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout=0.0):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=n_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if n_layers>1 else 0.0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len=1, input_dim)\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim, device=x.device)\n",
    "        c0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim, device=x.device)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        # out shape: (batch_size, seq_len, hidden_dim)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout=0.0):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=n_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if n_layers>1 else 0.0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim, device=x.device)\n",
    "\n",
    "        out, hn = self.gru(x, h0)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dl_model(model_name, input_dim, params):\n",
    "    \"\"\"\n",
    "    Builds a PyTorch model instance given model_name and hyperparams.\n",
    "    model_name: 'MLP', 'RNN', 'LSTM', 'GRU'\n",
    "    params:\n",
    "      - 'hidden_dim': dimension of hidden layer(s)\n",
    "      - 'n_layers': number of layers (2,3,4)\n",
    "      - 'dropout': dropout rate\n",
    "      ...\n",
    "    \"\"\"\n",
    "    hidden_dim = params.get(\"hidden_dim\", 64)\n",
    "    n_layers   = params.get(\"n_layers\", 2)\n",
    "    dropout    = params.get(\"dropout\", 0.0)\n",
    "    \n",
    "    if model_name == \"MLP\":\n",
    "        model = MLP(input_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "    elif model_name == \"RNN\":\n",
    "        model = RNN(input_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "    elif model_name == \"LSTM\":\n",
    "        model = LSTM(input_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "    elif model_name == \"GRU\":\n",
    "        model = GRU(input_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_name}\")\n",
    "    \n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_forecast_with_torch(\n",
    "    df,\n",
    "    target_col,\n",
    "    model_name,\n",
    "    model_params,\n",
    "    window_size=25,\n",
    "    test_ratio=0.2,\n",
    "    drop_cols=None,\n",
    "    log_dir=\"model_dl_logs\",\n",
    "    log_filename=\"prediction_log.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Rolls a window of size window_size through the data.\n",
    "    For each window:\n",
    "      - split train/test by test_ratio\n",
    "      - standard scale X and y\n",
    "      - train a PyTorch model\n",
    "      - predict and inverse scale\n",
    "      - log metrics (MSE, MAPE, R²)\n",
    "    \"\"\"\n",
    "    if drop_cols is None:\n",
    "        drop_cols = []\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    log_path = os.path.join(log_dir, log_filename)\n",
    "    log_entries = []\n",
    "\n",
    "    df = df.dropna().sort_index()\n",
    "    n = len(df)\n",
    "    \n",
    "    for start in range(0, n - window_size + 1):\n",
    "        window_df = df.iloc[start:start + window_size].copy()\n",
    "        X_window = window_df.drop(columns=[target_col] + drop_cols, errors='ignore')\n",
    "        y_window = window_df[target_col].values\n",
    "\n",
    "        train_size = int(len(X_window) * (1 - test_ratio))\n",
    "        if train_size < 1 or train_size >= len(X_window):\n",
    "            continue\n",
    "        \n",
    "        X_train = X_window.iloc[:train_size].values\n",
    "        X_test  = X_window.iloc[train_size:].values\n",
    "        y_train = y_window[:train_size]\n",
    "        y_test  = y_window[train_size:]\n",
    "\n",
    "        # Scale features\n",
    "        scaler_X = StandardScaler()\n",
    "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "        X_test_scaled  = scaler_X.transform(X_test)\n",
    "\n",
    "        # Scale target\n",
    "        scaler_y = StandardScaler()\n",
    "        y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "        y_test_scaled  = scaler_y.transform(y_test.reshape(-1, 1)).ravel()\n",
    "\n",
    "        # Format for model\n",
    "        input_dim = X_train_scaled.shape[1]\n",
    "        if model_name in [\"RNN\", \"LSTM\", \"GRU\"]:\n",
    "            X_train_scaled = np.expand_dims(X_train_scaled, axis=1)\n",
    "            X_test_scaled = np.expand_dims(X_test_scaled, axis=1)\n",
    "\n",
    "        X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "        y_train_t = torch.tensor(y_train_scaled, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        X_test_t = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "        y_test_t = torch.tensor(y_test_scaled, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "        # Build and train model\n",
    "        model = build_dl_model(model_name, input_dim, model_params)\n",
    "        lr = model_params.get(\"learning_rate\", 1e-3)\n",
    "        optimizer = model_params.get(\"optimizer\", \"adam\")\n",
    "        epochs = model_params.get(\"epochs\", 20)\n",
    "        batch_size = model_params.get(\"batch_size\", 32)\n",
    "\n",
    "        if optimizer.lower() == \"adam\":\n",
    "            opt = optim.Adam(model.parameters(), lr=lr)\n",
    "        else:\n",
    "            opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        dataset_size = X_train_t.shape[0]\n",
    "        num_batches = (dataset_size + batch_size - 1) // batch_size\n",
    "        model.train()\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            perm = torch.randperm(dataset_size)\n",
    "            X_train_t = X_train_t[perm]\n",
    "            y_train_t = y_train_t[perm]\n",
    "\n",
    "            for b_idx in range(num_batches):\n",
    "                start_b = b_idx * batch_size\n",
    "                end_b = min(start_b + batch_size, dataset_size)\n",
    "                xb = X_train_t[start_b:end_b]\n",
    "                yb = y_train_t[start_b:end_b]\n",
    "\n",
    "                opt.zero_grad()\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "        # Inference\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds_test = model(X_test_t)\n",
    "        preds_test = preds_test.cpu().numpy().ravel()\n",
    "        y_true_test = y_test_t.cpu().numpy().ravel()\n",
    "\n",
    "        # Inverse transform\n",
    "        preds_test = scaler_y.inverse_transform(preds_test.reshape(-1, 1)).ravel()\n",
    "        y_true_test = scaler_y.inverse_transform(y_true_test.reshape(-1, 1)).ravel()\n",
    "\n",
    "        # Guard against invalid output\n",
    "        if np.any(np.isnan(preds_test)) or np.any(np.isinf(preds_test)):\n",
    "            print(\"⚠️ Skipping window due to NaN/inf in predictions.\")\n",
    "            continue\n",
    "\n",
    "        mse = mean_squared_error(y_true_test, preds_test)\n",
    "        mape = mean_absolute_percentage_error(y_true_test, preds_test)\n",
    "        r2 = r2_score(y_true_test, preds_test)\n",
    "\n",
    "        log_entry = {\n",
    "            \"model_name\": model_name,\n",
    "            \"model_hyperparameters_dict\": json.dumps(model_params),\n",
    "            \"window_size\": window_size,\n",
    "            \"test_ratio\": test_ratio,\n",
    "            \"start_date\": str(df.index[start]),\n",
    "            \"end_date\": str(df.index[start + window_size - 1]),\n",
    "            \"test_data_values_list\": y_true_test.tolist(),\n",
    "            \"test_data_model_predictions_list\": preds_test.tolist(),\n",
    "            \"MSE_score\": mse,\n",
    "            \"MAPE_score\": mape,\n",
    "            \"R^2_score\": r2\n",
    "        }\n",
    "        log_entries.append(log_entry)\n",
    "\n",
    "    log_df = pd.DataFrame(log_entries)\n",
    "    if os.path.exists(log_path):\n",
    "        existing = pd.read_csv(log_path)\n",
    "        log_df = pd.concat([existing, log_df], ignore_index=True)\n",
    "    log_df.to_csv(log_path, index=False)\n",
    "    return log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_torch_model(\n",
    "    df,\n",
    "    target_col,\n",
    "    model_type,\n",
    "    param_grid,\n",
    "    window_sizes=[25, 50],\n",
    "    test_ratio=0.2,\n",
    "    drop_cols=None,\n",
    "    log_dir=\"model_dl_logs\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Similar approach as classical ML tune_model. \n",
    "    We'll iterate over param_grid, run sliding_window_forecast_with_torch, \n",
    "    log results, average, and return summary.\n",
    "    \"\"\"\n",
    "    if drop_cols is None:\n",
    "        drop_cols = []\n",
    "    \n",
    "    param_list = list(ParameterGrid(param_grid))\n",
    "    total_iterations = len(param_list) * len(window_sizes)\n",
    "    pbar = tqdm(total=total_iterations, desc=f\"Tuning {model_type}\")\n",
    "    \n",
    "    summaries = []\n",
    "    for w in window_sizes:\n",
    "        for params in param_list:\n",
    "            log_df = sliding_window_forecast_with_torch(\n",
    "                df=df,\n",
    "                target_col=target_col,\n",
    "                model_name=model_type,\n",
    "                model_params=params,\n",
    "                window_size=w,\n",
    "                test_ratio=test_ratio,\n",
    "                drop_cols=drop_cols,\n",
    "                log_dir=log_dir,\n",
    "                log_filename=f\"{model_type}_window_{w}.csv\"\n",
    "            )\n",
    "            \n",
    "            pbar.update(1)\n",
    "            if log_df.empty:\n",
    "                continue\n",
    "\n",
    "            avg_mse  = log_df[\"MSE_score\"].mean()\n",
    "            avg_mape = log_df[\"MAPE_score\"].mean()\n",
    "            avg_r2   = log_df[\"R^2_score\"].mean()\n",
    "\n",
    "            summary_entry = {\n",
    "                \"model_name\": model_type,\n",
    "                \"model_hyperparameters_dict\": json.dumps(params),\n",
    "                \"window_size\": w,\n",
    "                \"test_ratio\": test_ratio,\n",
    "                \"avg_MSE\": avg_mse,\n",
    "                \"avg_MAPE\": avg_mape,\n",
    "                \"avg_R^2\": avg_r2\n",
    "            }\n",
    "            summaries.append(summary_entry)\n",
    "    pbar.close()\n",
    "    \n",
    "    if summaries:\n",
    "        return pd.DataFrame(summaries)\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_top_logs(log_dir=\"model_dl_logs\", tops=5):\n",
    "    \"\"\"\n",
    "    Reads all CSV files in the given log directory, \n",
    "    groups by model_name and hyperparameters, \n",
    "    picks top \"tops\" by MAPE ascending for each model.\n",
    "    \"\"\"\n",
    "    all_files = [f for f in os.listdir(log_dir) if f.endswith(\".csv\")]\n",
    "    if not all_files:\n",
    "        print(f\"No CSV log files in {log_dir} directory.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    dfs = [pd.read_csv(os.path.join(log_dir, f)) for f in all_files]\n",
    "    combined_logs = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    grouped_summary = combined_logs.groupby(\n",
    "        [\"model_name\",\"model_hyperparameters_dict\"]\n",
    "    )[[\"MSE_score\",\"MAPE_score\",\"R^2_score\"]].mean().reset_index()\n",
    "\n",
    "    top_n_list = []\n",
    "    for model in grouped_summary[\"model_name\"].unique():\n",
    "        sub = grouped_summary[grouped_summary[\"model_name\"] == model]\n",
    "        top_n = sub.sort_values(\"MAPE_score\", ascending=True).head(tops)\n",
    "        top_n_list.append(top_n)\n",
    "    top_n_combined = pd.concat(top_n_list, ignore_index=True)\n",
    "    return top_n_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids_torch = {\n",
    "    \"MLP\": {\n",
    "        \"hidden_dim\":    [32, 64, 128],\n",
    "        \"n_layers\":      [1, 2, 3],\n",
    "        \"dropout\":       [0.0, 0.2, 0.4],                # Regularization\n",
    "        \"learning_rate\": [1e-2, 1e-3, 1e-4],             # Wider range\n",
    "        \"optimizer\":     [\"adam\"],\n",
    "        \"epochs\":        [25, 50, 100],                  # Reasonable ranges\n",
    "        \"batch_size\":    [16, 32, 64],\n",
    "        \"weight_decay\":  [0.0, 1e-4, 1e-3],              # L2 regularization\n",
    "    },\n",
    "\n",
    "    \"RNN\": {\n",
    "        \"hidden_dim\":    [32, 64, 128],\n",
    "        \"n_layers\":      [1, 2, 3],\n",
    "        \"dropout\":       [0.0, 0.2, 0.4], \n",
    "        \"learning_rate\": [1e-2, 1e-3, 1e-4],\n",
    "        \"optimizer\":     [\"adam\"],\n",
    "        \"epochs\":        [25, 50, 100],\n",
    "        \"batch_size\":    [16, 32, 64],\n",
    "        \"weight_decay\":  [0.0, 1e-4, 1e-3]\n",
    "    },\n",
    "\n",
    "    \"LSTM\": {\n",
    "        \"hidden_dim\":    [64, 128, 256],\n",
    "        \"n_layers\":      [1, 2, 3],\n",
    "        \"dropout\":       [0.0, 0.2, 0.4], \n",
    "        \"learning_rate\": [1e-2, 1e-3, 1e-4],\n",
    "        \"optimizer\":     [\"adam\"],\n",
    "        \"epochs\":        [25, 50, 100],\n",
    "        \"batch_size\":    [16, 32, 64],\n",
    "        \"weight_decay\":  [0.0, 1e-4, 1e-3]\n",
    "    },\n",
    "\n",
    "    \"GRU\": {\n",
    "        \"hidden_dim\":    [64, 128, 256],\n",
    "        \"n_layers\":      [1, 2, 3],\n",
    "        \"dropout\":       [0.0, 0.2, 0.4], \n",
    "        \"learning_rate\": [1e-2, 1e-3, 1e-4],\n",
    "        \"optimizer\":     [\"adam\"],\n",
    "        \"epochs\":        [25, 50, 100],\n",
    "        \"batch_size\":    [16, 32, 64],\n",
    "        \"weight_decay\":  [0.0, 1e-4, 1e-3]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['AAPL', 'MSFT', 'LLY', 'UNH', 'V', 'MA', 'GOOGL', 'META', 'AMZN', 'TSLA', 'PG', 'WMT', 'RTX', 'UNP', 'XOM', 'CVX', 'LIN', 'SHW', 'AMT', 'PLD', 'NEE', 'SO'])\n"
     ]
    }
   ],
   "source": [
    "with open(\"temp_output/quarterly_X_y.pkl\", \"rb\") as f:\n",
    "    data_q = pickle.load(f)\n",
    "\n",
    "drop_columns = []\n",
    "print(data_q.keys())  # Should show all the tickers like AAPL, MSFT, etc.\n",
    "quaterly_data = data_q[\"AAPL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning MLP: 100%|██████████| 2187/2187 [2:05:42<00:00,  3.45s/it]  \n",
      "Tuning RNN: 100%|██████████| 2187/2187 [2:40:41<00:00,  4.41s/it]  \n",
      "Tuning LSTM: 100%|██████████| 2187/2187 [3:15:22<00:00,  5.36s/it]  \n",
      "Tuning GRU: 100%|██████████| 2187/2187 [3:07:10<00:00,  5.14s/it]  \n"
     ]
    }
   ],
   "source": [
    "# MLP on quarterly\n",
    "mlp_summary_q = tune_torch_model(\n",
    "    df=quaterly_data,\n",
    "    target_col=\"y\",\n",
    "    model_type=\"MLP\",\n",
    "    param_grid=param_grids_torch[\"MLP\"],\n",
    "    window_sizes=[10],  # or [25,50]\n",
    "    test_ratio=0.2,\n",
    "    drop_cols=drop_columns,\n",
    "    log_dir=\"model_dl_logs_q\"\n",
    ")\n",
    "\n",
    "# RNN on quarterly\n",
    "rnn_summary_q = tune_torch_model(\n",
    "    df=quaterly_data,\n",
    "    target_col=\"y\",\n",
    "    model_type=\"RNN\",\n",
    "    param_grid=param_grids_torch[\"RNN\"],\n",
    "    window_sizes=[10],\n",
    "    test_ratio=0.2,\n",
    "    drop_cols=drop_columns,\n",
    "    log_dir=\"model_dl_logs_q\"\n",
    ")\n",
    "\n",
    "# LSTM on quarterly\n",
    "lstm_summary_q = tune_torch_model(\n",
    "    df=quaterly_data,\n",
    "    target_col=\"y\",\n",
    "    model_type=\"LSTM\",\n",
    "    param_grid=param_grids_torch[\"LSTM\"],\n",
    "    window_sizes=[10],\n",
    "    test_ratio=0.2,\n",
    "    drop_cols=drop_columns,\n",
    "    log_dir=\"model_dl_logs_q\"\n",
    ")\n",
    "\n",
    "# GRU on quarterly\n",
    "gru_summary_q = tune_torch_model(\n",
    "    df=quaterly_data,\n",
    "    target_col=\"y\",\n",
    "    model_type=\"GRU\",\n",
    "    param_grid=param_grids_torch[\"GRU\"],\n",
    "    window_sizes=[10],\n",
    "    test_ratio=0.2,\n",
    "    drop_cols=drop_columns,\n",
    "    log_dir=\"model_dl_logs_q\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Tuning Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_hyperparameters_dict</th>\n",
       "      <th>window_size</th>\n",
       "      <th>test_ratio</th>\n",
       "      <th>avg_MSE</th>\n",
       "      <th>avg_MAPE</th>\n",
       "      <th>avg_R^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP</td>\n",
       "      <td>{\"batch_size\": 16, \"dropout\": 0.0, \"epochs\": 2...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>778.982207</td>\n",
       "      <td>0.177793</td>\n",
       "      <td>-183.826692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP</td>\n",
       "      <td>{\"batch_size\": 16, \"dropout\": 0.0, \"epochs\": 2...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>769.949728</td>\n",
       "      <td>0.176999</td>\n",
       "      <td>-200.679550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>{\"batch_size\": 16, \"dropout\": 0.0, \"epochs\": 2...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>835.726052</td>\n",
       "      <td>0.181807</td>\n",
       "      <td>-198.160551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP</td>\n",
       "      <td>{\"batch_size\": 16, \"dropout\": 0.0, \"epochs\": 2...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>798.616589</td>\n",
       "      <td>0.180312</td>\n",
       "      <td>-177.330999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP</td>\n",
       "      <td>{\"batch_size\": 16, \"dropout\": 0.0, \"epochs\": 2...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>787.044281</td>\n",
       "      <td>0.179981</td>\n",
       "      <td>-173.335006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8743</th>\n",
       "      <td>GRU</td>\n",
       "      <td>{\"batch_size\": 64, \"dropout\": 0.4, \"epochs\": 1...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>816.831696</td>\n",
       "      <td>0.183870</td>\n",
       "      <td>-220.677119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8744</th>\n",
       "      <td>GRU</td>\n",
       "      <td>{\"batch_size\": 64, \"dropout\": 0.4, \"epochs\": 1...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>816.741247</td>\n",
       "      <td>0.183856</td>\n",
       "      <td>-220.627070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8745</th>\n",
       "      <td>GRU</td>\n",
       "      <td>{\"batch_size\": 64, \"dropout\": 0.4, \"epochs\": 1...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>816.653085</td>\n",
       "      <td>0.183842</td>\n",
       "      <td>-220.581051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8746</th>\n",
       "      <td>GRU</td>\n",
       "      <td>{\"batch_size\": 64, \"dropout\": 0.4, \"epochs\": 1...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>816.567877</td>\n",
       "      <td>0.183829</td>\n",
       "      <td>-220.532425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8747</th>\n",
       "      <td>GRU</td>\n",
       "      <td>{\"batch_size\": 64, \"dropout\": 0.4, \"epochs\": 1...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>816.473666</td>\n",
       "      <td>0.183814</td>\n",
       "      <td>-220.487105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8748 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     model_name                         model_hyperparameters_dict  \\\n",
       "0           MLP  {\"batch_size\": 16, \"dropout\": 0.0, \"epochs\": 2...   \n",
       "1           MLP  {\"batch_size\": 16, \"dropout\": 0.0, \"epochs\": 2...   \n",
       "2           MLP  {\"batch_size\": 16, \"dropout\": 0.0, \"epochs\": 2...   \n",
       "3           MLP  {\"batch_size\": 16, \"dropout\": 0.0, \"epochs\": 2...   \n",
       "4           MLP  {\"batch_size\": 16, \"dropout\": 0.0, \"epochs\": 2...   \n",
       "...         ...                                                ...   \n",
       "8743        GRU  {\"batch_size\": 64, \"dropout\": 0.4, \"epochs\": 1...   \n",
       "8744        GRU  {\"batch_size\": 64, \"dropout\": 0.4, \"epochs\": 1...   \n",
       "8745        GRU  {\"batch_size\": 64, \"dropout\": 0.4, \"epochs\": 1...   \n",
       "8746        GRU  {\"batch_size\": 64, \"dropout\": 0.4, \"epochs\": 1...   \n",
       "8747        GRU  {\"batch_size\": 64, \"dropout\": 0.4, \"epochs\": 1...   \n",
       "\n",
       "      window_size  test_ratio     avg_MSE  avg_MAPE     avg_R^2  \n",
       "0              10         0.2  778.982207  0.177793 -183.826692  \n",
       "1              10         0.2  769.949728  0.176999 -200.679550  \n",
       "2              10         0.2  835.726052  0.181807 -198.160551  \n",
       "3              10         0.2  798.616589  0.180312 -177.330999  \n",
       "4              10         0.2  787.044281  0.179981 -173.335006  \n",
       "...           ...         ...         ...       ...         ...  \n",
       "8743           10         0.2  816.831696  0.183870 -220.677119  \n",
       "8744           10         0.2  816.741247  0.183856 -220.627070  \n",
       "8745           10         0.2  816.653085  0.183842 -220.581051  \n",
       "8746           10         0.2  816.567877  0.183829 -220.532425  \n",
       "8747           10         0.2  816.473666  0.183814 -220.487105  \n",
       "\n",
       "[8748 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Configurations per Model:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_hyperparameters_dict</th>\n",
       "      <th>MSE_score</th>\n",
       "      <th>MAPE_score</th>\n",
       "      <th>R^2_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRU</td>\n",
       "      <td>{\"batch_size\": 16, \"dropout\": 0.4, \"epochs\": 2...</td>\n",
       "      <td>604.429245</td>\n",
       "      <td>0.148500</td>\n",
       "      <td>-114.641782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GRU</td>\n",
       "      <td>{\"batch_size\": 32, \"dropout\": 0.4, \"epochs\": 2...</td>\n",
       "      <td>624.912527</td>\n",
       "      <td>0.149950</td>\n",
       "      <td>-118.385994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GRU</td>\n",
       "      <td>{\"batch_size\": 16, \"dropout\": 0.4, \"epochs\": 2...</td>\n",
       "      <td>638.081727</td>\n",
       "      <td>0.150358</td>\n",
       "      <td>-134.695980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GRU</td>\n",
       "      <td>{\"batch_size\": 16, \"dropout\": 0.4, \"epochs\": 1...</td>\n",
       "      <td>604.974328</td>\n",
       "      <td>0.150906</td>\n",
       "      <td>-120.426897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRU</td>\n",
       "      <td>{\"batch_size\": 32, \"dropout\": 0.4, \"epochs\": 1...</td>\n",
       "      <td>599.377000</td>\n",
       "      <td>0.150942</td>\n",
       "      <td>-108.714437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>{\"batch_size\": 16, \"dropout\": 0.0, \"epochs\": 2...</td>\n",
       "      <td>592.196254</td>\n",
       "      <td>0.145152</td>\n",
       "      <td>-73.316614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>{\"batch_size\": 32, \"dropout\": 0.4, \"epochs\": 2...</td>\n",
       "      <td>603.333849</td>\n",
       "      <td>0.147617</td>\n",
       "      <td>-169.880725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>{\"batch_size\": 16, \"dropout\": 0.0, \"epochs\": 2...</td>\n",
       "      <td>592.335330</td>\n",
       "      <td>0.148016</td>\n",
       "      <td>-82.748094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>{\"batch_size\": 16, \"dropout\": 0.4, \"epochs\": 2...</td>\n",
       "      <td>602.140783</td>\n",
       "      <td>0.148135</td>\n",
       "      <td>-153.402797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>{\"batch_size\": 64, \"dropout\": 0.4, \"epochs\": 2...</td>\n",
       "      <td>589.238387</td>\n",
       "      <td>0.148434</td>\n",
       "      <td>-160.610464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MLP</td>\n",
       "      <td>{\"batch_size\": 16, \"dropout\": 0.2, \"epochs\": 5...</td>\n",
       "      <td>558.754841</td>\n",
       "      <td>0.141765</td>\n",
       "      <td>-36.421118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MLP</td>\n",
       "      <td>{\"batch_size\": 32, \"dropout\": 0.4, \"epochs\": 2...</td>\n",
       "      <td>598.458838</td>\n",
       "      <td>0.143225</td>\n",
       "      <td>-156.047660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MLP</td>\n",
       "      <td>{\"batch_size\": 32, \"dropout\": 0.0, \"epochs\": 1...</td>\n",
       "      <td>530.521045</td>\n",
       "      <td>0.143675</td>\n",
       "      <td>-91.399629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MLP</td>\n",
       "      <td>{\"batch_size\": 16, \"dropout\": 0.2, \"epochs\": 2...</td>\n",
       "      <td>552.902606</td>\n",
       "      <td>0.144727</td>\n",
       "      <td>-90.299266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MLP</td>\n",
       "      <td>{\"batch_size\": 32, \"dropout\": 0.4, \"epochs\": 2...</td>\n",
       "      <td>539.727484</td>\n",
       "      <td>0.145214</td>\n",
       "      <td>-166.377211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RNN</td>\n",
       "      <td>{\"batch_size\": 32, \"dropout\": 0.2, \"epochs\": 1...</td>\n",
       "      <td>530.859115</td>\n",
       "      <td>0.148577</td>\n",
       "      <td>-152.204564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RNN</td>\n",
       "      <td>{\"batch_size\": 32, \"dropout\": 0.2, \"epochs\": 5...</td>\n",
       "      <td>555.325703</td>\n",
       "      <td>0.149120</td>\n",
       "      <td>-171.250566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RNN</td>\n",
       "      <td>{\"batch_size\": 16, \"dropout\": 0.4, \"epochs\": 1...</td>\n",
       "      <td>546.549843</td>\n",
       "      <td>0.149759</td>\n",
       "      <td>-166.173312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RNN</td>\n",
       "      <td>{\"batch_size\": 32, \"dropout\": 0.2, \"epochs\": 1...</td>\n",
       "      <td>510.058704</td>\n",
       "      <td>0.150072</td>\n",
       "      <td>-163.028414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RNN</td>\n",
       "      <td>{\"batch_size\": 32, \"dropout\": 0.4, \"epochs\": 1...</td>\n",
       "      <td>581.488683</td>\n",
       "      <td>0.150560</td>\n",
       "      <td>-149.524961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_name                         model_hyperparameters_dict   MSE_score  \\\n",
       "0         GRU  {\"batch_size\": 16, \"dropout\": 0.4, \"epochs\": 2...  604.429245   \n",
       "1         GRU  {\"batch_size\": 32, \"dropout\": 0.4, \"epochs\": 2...  624.912527   \n",
       "2         GRU  {\"batch_size\": 16, \"dropout\": 0.4, \"epochs\": 2...  638.081727   \n",
       "3         GRU  {\"batch_size\": 16, \"dropout\": 0.4, \"epochs\": 1...  604.974328   \n",
       "4         GRU  {\"batch_size\": 32, \"dropout\": 0.4, \"epochs\": 1...  599.377000   \n",
       "5        LSTM  {\"batch_size\": 16, \"dropout\": 0.0, \"epochs\": 2...  592.196254   \n",
       "6        LSTM  {\"batch_size\": 32, \"dropout\": 0.4, \"epochs\": 2...  603.333849   \n",
       "7        LSTM  {\"batch_size\": 16, \"dropout\": 0.0, \"epochs\": 2...  592.335330   \n",
       "8        LSTM  {\"batch_size\": 16, \"dropout\": 0.4, \"epochs\": 2...  602.140783   \n",
       "9        LSTM  {\"batch_size\": 64, \"dropout\": 0.4, \"epochs\": 2...  589.238387   \n",
       "10        MLP  {\"batch_size\": 16, \"dropout\": 0.2, \"epochs\": 5...  558.754841   \n",
       "11        MLP  {\"batch_size\": 32, \"dropout\": 0.4, \"epochs\": 2...  598.458838   \n",
       "12        MLP  {\"batch_size\": 32, \"dropout\": 0.0, \"epochs\": 1...  530.521045   \n",
       "13        MLP  {\"batch_size\": 16, \"dropout\": 0.2, \"epochs\": 2...  552.902606   \n",
       "14        MLP  {\"batch_size\": 32, \"dropout\": 0.4, \"epochs\": 2...  539.727484   \n",
       "15        RNN  {\"batch_size\": 32, \"dropout\": 0.2, \"epochs\": 1...  530.859115   \n",
       "16        RNN  {\"batch_size\": 32, \"dropout\": 0.2, \"epochs\": 5...  555.325703   \n",
       "17        RNN  {\"batch_size\": 16, \"dropout\": 0.4, \"epochs\": 1...  546.549843   \n",
       "18        RNN  {\"batch_size\": 32, \"dropout\": 0.2, \"epochs\": 1...  510.058704   \n",
       "19        RNN  {\"batch_size\": 32, \"dropout\": 0.4, \"epochs\": 1...  581.488683   \n",
       "\n",
       "    MAPE_score   R^2_score  \n",
       "0     0.148500 -114.641782  \n",
       "1     0.149950 -118.385994  \n",
       "2     0.150358 -134.695980  \n",
       "3     0.150906 -120.426897  \n",
       "4     0.150942 -108.714437  \n",
       "5     0.145152  -73.316614  \n",
       "6     0.147617 -169.880725  \n",
       "7     0.148016  -82.748094  \n",
       "8     0.148135 -153.402797  \n",
       "9     0.148434 -160.610464  \n",
       "10    0.141765  -36.421118  \n",
       "11    0.143225 -156.047660  \n",
       "12    0.143675  -91.399629  \n",
       "13    0.144727  -90.299266  \n",
       "14    0.145214 -166.377211  \n",
       "15    0.148577 -152.204564  \n",
       "16    0.149120 -171.250566  \n",
       "17    0.149759 -166.173312  \n",
       "18    0.150072 -163.028414  \n",
       "19    0.150560 -149.524961  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_summary_q = pd.concat([mlp_summary_q, rnn_summary_q, lstm_summary_q, gru_summary_q], ignore_index=True)\n",
    "print(\"Combined Tuning Summary:\")\n",
    "display(combined_summary_q)\n",
    "\n",
    "# Alternatively, read all prediction log files from log directory\n",
    "top5_summary_q = combine_and_top_logs(log_dir=\"model_dl_logs_q\")\n",
    "print(\"Top 5 Configurations per Model:\")\n",
    "display(top5_summary_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['AAPL', 'MSFT', 'LLY', 'UNH', 'V', 'MA', 'GOOGL', 'META', 'AMZN', 'TSLA', 'PG', 'WMT', 'RTX', 'UNP', 'XOM', 'CVX', 'LIN', 'SHW', 'AMT', 'PLD', 'NEE', 'SO', '^GSPC'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>SMA5</th>\n",
       "      <th>SMA50</th>\n",
       "      <th>SMA200</th>\n",
       "      <th>MACDLine</th>\n",
       "      <th>...</th>\n",
       "      <th>^IXIC</th>\n",
       "      <th>^DJI</th>\n",
       "      <th>^VIX</th>\n",
       "      <th>CL=F</th>\n",
       "      <th>GC=F</th>\n",
       "      <th>SI=F</th>\n",
       "      <th>^TNX</th>\n",
       "      <th>DX-Y.NYB</th>\n",
       "      <th>FedFundsRate</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-07-01</th>\n",
       "      <td>216.023956</td>\n",
       "      <td>216.750000</td>\n",
       "      <td>217.509995</td>\n",
       "      <td>211.919998</td>\n",
       "      <td>212.089996</td>\n",
       "      <td>60402900</td>\n",
       "      <td>212.045334</td>\n",
       "      <td>190.963962</td>\n",
       "      <td>182.925126</td>\n",
       "      <td>6.332360</td>\n",
       "      <td>...</td>\n",
       "      <td>17879.300781</td>\n",
       "      <td>39169.519531</td>\n",
       "      <td>12.22</td>\n",
       "      <td>83.379997</td>\n",
       "      <td>2327.600098</td>\n",
       "      <td>29.299999</td>\n",
       "      <td>4.479</td>\n",
       "      <td>105.900002</td>\n",
       "      <td>5.33</td>\n",
       "      <td>219.532166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-02</th>\n",
       "      <td>219.532166</td>\n",
       "      <td>220.270004</td>\n",
       "      <td>220.380005</td>\n",
       "      <td>215.100006</td>\n",
       "      <td>216.149994</td>\n",
       "      <td>58046200</td>\n",
       "      <td>214.277829</td>\n",
       "      <td>192.070114</td>\n",
       "      <td>183.150480</td>\n",
       "      <td>6.644230</td>\n",
       "      <td>...</td>\n",
       "      <td>18028.759766</td>\n",
       "      <td>39331.851562</td>\n",
       "      <td>12.03</td>\n",
       "      <td>82.809998</td>\n",
       "      <td>2323.000000</td>\n",
       "      <td>29.353001</td>\n",
       "      <td>4.436</td>\n",
       "      <td>105.720001</td>\n",
       "      <td>5.33</td>\n",
       "      <td>220.807892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-03</th>\n",
       "      <td>220.807892</td>\n",
       "      <td>221.550003</td>\n",
       "      <td>221.550003</td>\n",
       "      <td>219.029999</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>37369800</td>\n",
       "      <td>215.932269</td>\n",
       "      <td>193.185060</td>\n",
       "      <td>183.385837</td>\n",
       "      <td>6.914623</td>\n",
       "      <td>...</td>\n",
       "      <td>18188.300781</td>\n",
       "      <td>39308.000000</td>\n",
       "      <td>12.09</td>\n",
       "      <td>83.879997</td>\n",
       "      <td>2359.800049</td>\n",
       "      <td>30.548000</td>\n",
       "      <td>4.355</td>\n",
       "      <td>105.400002</td>\n",
       "      <td>5.33</td>\n",
       "      <td>225.581833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-05</th>\n",
       "      <td>225.581833</td>\n",
       "      <td>226.339996</td>\n",
       "      <td>226.449997</td>\n",
       "      <td>221.649994</td>\n",
       "      <td>221.649994</td>\n",
       "      <td>60412400</td>\n",
       "      <td>218.372064</td>\n",
       "      <td>194.374384</td>\n",
       "      <td>183.630370</td>\n",
       "      <td>7.428497</td>\n",
       "      <td>...</td>\n",
       "      <td>18352.759766</td>\n",
       "      <td>39375.871094</td>\n",
       "      <td>12.48</td>\n",
       "      <td>83.160004</td>\n",
       "      <td>2388.500000</td>\n",
       "      <td>31.388000</td>\n",
       "      <td>4.272</td>\n",
       "      <td>104.879997</td>\n",
       "      <td>5.33</td>\n",
       "      <td>227.056885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-08</th>\n",
       "      <td>227.056885</td>\n",
       "      <td>227.820007</td>\n",
       "      <td>227.850006</td>\n",
       "      <td>223.250000</td>\n",
       "      <td>227.089996</td>\n",
       "      <td>59085900</td>\n",
       "      <td>221.800546</td>\n",
       "      <td>195.551008</td>\n",
       "      <td>183.876819</td>\n",
       "      <td>7.864117</td>\n",
       "      <td>...</td>\n",
       "      <td>18403.740234</td>\n",
       "      <td>39344.789062</td>\n",
       "      <td>12.37</td>\n",
       "      <td>82.330002</td>\n",
       "      <td>2355.199951</td>\n",
       "      <td>30.618000</td>\n",
       "      <td>4.269</td>\n",
       "      <td>105.010002</td>\n",
       "      <td>5.33</td>\n",
       "      <td>227.913986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-24</th>\n",
       "      <td>257.916443</td>\n",
       "      <td>258.200012</td>\n",
       "      <td>258.209991</td>\n",
       "      <td>255.289993</td>\n",
       "      <td>255.490005</td>\n",
       "      <td>23234700</td>\n",
       "      <td>252.881967</td>\n",
       "      <td>235.564016</td>\n",
       "      <td>211.182570</td>\n",
       "      <td>6.067327</td>\n",
       "      <td>...</td>\n",
       "      <td>20031.130859</td>\n",
       "      <td>43297.031250</td>\n",
       "      <td>14.27</td>\n",
       "      <td>70.099998</td>\n",
       "      <td>2620.000000</td>\n",
       "      <td>29.974001</td>\n",
       "      <td>4.591</td>\n",
       "      <td>108.260002</td>\n",
       "      <td>4.33</td>\n",
       "      <td>258.735504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-26</th>\n",
       "      <td>258.735504</td>\n",
       "      <td>259.019989</td>\n",
       "      <td>260.100006</td>\n",
       "      <td>257.630005</td>\n",
       "      <td>258.190002</td>\n",
       "      <td>27237100</td>\n",
       "      <td>255.073553</td>\n",
       "      <td>236.071997</td>\n",
       "      <td>211.614168</td>\n",
       "      <td>6.300019</td>\n",
       "      <td>...</td>\n",
       "      <td>20020.359375</td>\n",
       "      <td>43325.800781</td>\n",
       "      <td>14.73</td>\n",
       "      <td>69.620003</td>\n",
       "      <td>2638.800049</td>\n",
       "      <td>30.047001</td>\n",
       "      <td>4.579</td>\n",
       "      <td>108.129997</td>\n",
       "      <td>4.33</td>\n",
       "      <td>255.309296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-27</th>\n",
       "      <td>255.309296</td>\n",
       "      <td>255.589996</td>\n",
       "      <td>258.700012</td>\n",
       "      <td>253.059998</td>\n",
       "      <td>257.829987</td>\n",
       "      <td>42355300</td>\n",
       "      <td>256.232281</td>\n",
       "      <td>236.552763</td>\n",
       "      <td>212.039086</td>\n",
       "      <td>6.137217</td>\n",
       "      <td>...</td>\n",
       "      <td>19722.029297</td>\n",
       "      <td>42992.210938</td>\n",
       "      <td>15.95</td>\n",
       "      <td>70.599998</td>\n",
       "      <td>2617.199951</td>\n",
       "      <td>29.655001</td>\n",
       "      <td>4.619</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>4.33</td>\n",
       "      <td>251.923019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-30</th>\n",
       "      <td>251.923019</td>\n",
       "      <td>252.199997</td>\n",
       "      <td>253.500000</td>\n",
       "      <td>250.750000</td>\n",
       "      <td>252.229996</td>\n",
       "      <td>35557500</td>\n",
       "      <td>255.774783</td>\n",
       "      <td>236.958419</td>\n",
       "      <td>212.437766</td>\n",
       "      <td>5.669595</td>\n",
       "      <td>...</td>\n",
       "      <td>19486.789062</td>\n",
       "      <td>42573.730469</td>\n",
       "      <td>17.40</td>\n",
       "      <td>70.989998</td>\n",
       "      <td>2606.100098</td>\n",
       "      <td>29.106001</td>\n",
       "      <td>4.545</td>\n",
       "      <td>108.129997</td>\n",
       "      <td>4.33</td>\n",
       "      <td>250.144974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31</th>\n",
       "      <td>250.144974</td>\n",
       "      <td>250.419998</td>\n",
       "      <td>253.279999</td>\n",
       "      <td>249.429993</td>\n",
       "      <td>252.440002</td>\n",
       "      <td>39480700</td>\n",
       "      <td>254.805847</td>\n",
       "      <td>237.271641</td>\n",
       "      <td>212.829447</td>\n",
       "      <td>5.096776</td>\n",
       "      <td>...</td>\n",
       "      <td>19310.789062</td>\n",
       "      <td>42544.218750</td>\n",
       "      <td>17.35</td>\n",
       "      <td>71.720001</td>\n",
       "      <td>2629.199951</td>\n",
       "      <td>28.940001</td>\n",
       "      <td>4.573</td>\n",
       "      <td>108.489998</td>\n",
       "      <td>4.33</td>\n",
       "      <td>243.582199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Adj Close       Close        High         Low        Open  \\\n",
       "2024-07-01  216.023956  216.750000  217.509995  211.919998  212.089996   \n",
       "2024-07-02  219.532166  220.270004  220.380005  215.100006  216.149994   \n",
       "2024-07-03  220.807892  221.550003  221.550003  219.029999  220.000000   \n",
       "2024-07-05  225.581833  226.339996  226.449997  221.649994  221.649994   \n",
       "2024-07-08  227.056885  227.820007  227.850006  223.250000  227.089996   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2024-12-24  257.916443  258.200012  258.209991  255.289993  255.490005   \n",
       "2024-12-26  258.735504  259.019989  260.100006  257.630005  258.190002   \n",
       "2024-12-27  255.309296  255.589996  258.700012  253.059998  257.829987   \n",
       "2024-12-30  251.923019  252.199997  253.500000  250.750000  252.229996   \n",
       "2024-12-31  250.144974  250.419998  253.279999  249.429993  252.440002   \n",
       "\n",
       "              Volume        SMA5       SMA50      SMA200  MACDLine  ...  \\\n",
       "2024-07-01  60402900  212.045334  190.963962  182.925126  6.332360  ...   \n",
       "2024-07-02  58046200  214.277829  192.070114  183.150480  6.644230  ...   \n",
       "2024-07-03  37369800  215.932269  193.185060  183.385837  6.914623  ...   \n",
       "2024-07-05  60412400  218.372064  194.374384  183.630370  7.428497  ...   \n",
       "2024-07-08  59085900  221.800546  195.551008  183.876819  7.864117  ...   \n",
       "...              ...         ...         ...         ...       ...  ...   \n",
       "2024-12-24  23234700  252.881967  235.564016  211.182570  6.067327  ...   \n",
       "2024-12-26  27237100  255.073553  236.071997  211.614168  6.300019  ...   \n",
       "2024-12-27  42355300  256.232281  236.552763  212.039086  6.137217  ...   \n",
       "2024-12-30  35557500  255.774783  236.958419  212.437766  5.669595  ...   \n",
       "2024-12-31  39480700  254.805847  237.271641  212.829447  5.096776  ...   \n",
       "\n",
       "                   ^IXIC          ^DJI   ^VIX       CL=F         GC=F  \\\n",
       "2024-07-01  17879.300781  39169.519531  12.22  83.379997  2327.600098   \n",
       "2024-07-02  18028.759766  39331.851562  12.03  82.809998  2323.000000   \n",
       "2024-07-03  18188.300781  39308.000000  12.09  83.879997  2359.800049   \n",
       "2024-07-05  18352.759766  39375.871094  12.48  83.160004  2388.500000   \n",
       "2024-07-08  18403.740234  39344.789062  12.37  82.330002  2355.199951   \n",
       "...                  ...           ...    ...        ...          ...   \n",
       "2024-12-24  20031.130859  43297.031250  14.27  70.099998  2620.000000   \n",
       "2024-12-26  20020.359375  43325.800781  14.73  69.620003  2638.800049   \n",
       "2024-12-27  19722.029297  42992.210938  15.95  70.599998  2617.199951   \n",
       "2024-12-30  19486.789062  42573.730469  17.40  70.989998  2606.100098   \n",
       "2024-12-31  19310.789062  42544.218750  17.35  71.720001  2629.199951   \n",
       "\n",
       "                 SI=F   ^TNX    DX-Y.NYB  FedFundsRate           y  \n",
       "2024-07-01  29.299999  4.479  105.900002          5.33  219.532166  \n",
       "2024-07-02  29.353001  4.436  105.720001          5.33  220.807892  \n",
       "2024-07-03  30.548000  4.355  105.400002          5.33  225.581833  \n",
       "2024-07-05  31.388000  4.272  104.879997          5.33  227.056885  \n",
       "2024-07-08  30.618000  4.269  105.010002          5.33  227.913986  \n",
       "...               ...    ...         ...           ...         ...  \n",
       "2024-12-24  29.974001  4.591  108.260002          4.33  258.735504  \n",
       "2024-12-26  30.047001  4.579  108.129997          4.33  255.309296  \n",
       "2024-12-27  29.655001  4.619  108.000000          4.33  251.923019  \n",
       "2024-12-30  29.106001  4.545  108.129997          4.33  250.144974  \n",
       "2024-12-31  28.940001  4.573  108.489998          4.33  243.582199  \n",
       "\n",
       "[128 rows x 39 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"temp_output/daily_X_y.pkl\", \"rb\") as f:\n",
    "    data_d = pickle.load(f)\n",
    "    \n",
    "drop_columns = []\n",
    "print(data_d.keys())  # Should show all the tickers like AAPL, MSFT, etc.\n",
    "daily_data = data_d[\"AAPL\"]\n",
    "daily_data = daily_data.iloc[-128:,:]\n",
    "daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning MLP:  19%|█▊        | 410/2187 [2:00:23<15:03:59, 30.52s/it]"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "mlp_summary_d = tune_torch_model(\n",
    "    df=daily_data,\n",
    "    target_col=\"y\",\n",
    "    model_type=\"MLP\",\n",
    "    param_grid=param_grids_torch[\"MLP\"],\n",
    "    window_sizes=[25],\n",
    "    test_ratio=0.2,\n",
    "    drop_cols=drop_columns,\n",
    "    log_dir=\"model_dl_logs_d\"\n",
    ")\n",
    "\n",
    "# RNN\n",
    "rnn_summary_d = tune_torch_model(\n",
    "    df=daily_data,\n",
    "    target_col=\"y\",\n",
    "    model_type=\"RNN\",\n",
    "    param_grid=param_grids_torch[\"RNN\"],\n",
    "    window_sizes=[25],\n",
    "    test_ratio=0.2,\n",
    "    drop_cols=drop_columns,\n",
    "    log_dir=\"model_dl_logs_d\"\n",
    ")\n",
    "\n",
    "# LSTM\n",
    "lstm_summary_d = tune_torch_model(\n",
    "    df=daily_data,\n",
    "    target_col=\"y\",\n",
    "    model_type=\"LSTM\",\n",
    "    param_grid=param_grids_torch[\"LSTM\"],\n",
    "    window_sizes=[25],\n",
    "    test_ratio=0.2,\n",
    "    drop_cols=drop_columns,\n",
    "    log_dir=\"model_dl_logs_d\"\n",
    ")\n",
    "\n",
    "# GRU\n",
    "gru_summary_d = tune_torch_model(\n",
    "    df=daily_data,\n",
    "    target_col=\"y\",\n",
    "    model_type=\"GRU\",\n",
    "    param_grid=param_grids_torch[\"GRU\"],\n",
    "    window_sizes=[25],\n",
    "    test_ratio=0.2,\n",
    "    drop_cols=drop_columns,\n",
    "    log_dir=\"model_dl_logs_d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_summary_d = pd.concat([mlp_summary_d, rnn_summary_d, lstm_summary_d, gru_summary_d], ignore_index=True)\n",
    "print(\"Combined Tuning Summary:\")\n",
    "display(combined_summary_d)\n",
    "\n",
    "# Alternatively, read all prediction log files from log directory\n",
    "top5_summary_d = combine_and_top_logs(log_dir=\"model_dl_logs_d\")\n",
    "print(\"Top 5 Configurations per Model:\")\n",
    "display(top5_summary_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 8000}\n",
      "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 6400}\n",
      "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 4800}\n",
      "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 3200}\n",
      "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 1600}\n",
      "{'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 8000}\n",
      "{'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 6400}\n",
      "{'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 4800}\n",
      "{'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 3200}\n",
      "{'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 1600}\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 4800}\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 6400}\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 8000}\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 3200}\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1600}\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 1600}\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 4800}\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 6400}\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 8000}\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 3200}\n",
      "{'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 8000}\n",
      "{'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 6400}\n",
      "{'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 4800}\n",
      "{'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 3200}\n",
      "{'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 1600}\n",
      "{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 2000}\n",
      "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 2000}\n",
      "{'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 2000}\n",
      "{'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 2000}\n",
      "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 1600}\n",
      "{'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 1600}\n",
      "{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 1600}\n",
      "{'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 1600}\n",
      "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 1250}\n",
      "{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 1250}\n",
      "{'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 1250}\n",
      "{'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 1250}\n",
      "{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "{'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "{'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "{'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 800}\n",
      "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 800}\n",
      "{'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 800}\n",
      "{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 800}\n",
      "{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 640}\n",
      "{'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 640}\n",
      "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 640}\n",
      "{'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 640}\n",
      "{'max_depth': 12, 'min_samples_split': 3, 'n_estimators': 2000}\n",
      "{'C': 6, 'epsilon': 1.25e-05, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 2.5e-05, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 5e-05, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 0.0001, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 0.0002, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 0.0004, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 0.0008, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 0.0016, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 0.0032, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 0.0063, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 0.0125, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 0.025, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 0.05, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 0.1, 'kernel': 'linear'}\n",
      "{'C': 5.5, 'epsilon': 1.25e-05, 'kernel': 'linear'}\n",
      "{'C': 5.5, 'epsilon': 2.5e-05, 'kernel': 'linear'}\n",
      "{'C': 5.5, 'epsilon': 5e-05, 'kernel': 'linear'}\n",
      "{'C': 5.5, 'epsilon': 0.0001, 'kernel': 'linear'}\n",
      "{'C': 5.5, 'epsilon': 0.0002, 'kernel': 'linear'}\n",
      "{'C': 5.5, 'epsilon': 0.0004, 'kernel': 'linear'}\n",
      "{'C': 5.5, 'epsilon': 0.0008, 'kernel': 'linear'}\n",
      "{'C': 5.5, 'epsilon': 0.0016, 'kernel': 'linear'}\n",
      "{'C': 5.5, 'epsilon': 0.0032, 'kernel': 'linear'}\n",
      "{'C': 5.5, 'epsilon': 0.0063, 'kernel': 'linear'}\n",
      "{'C': 5.5, 'epsilon': 0.0125, 'kernel': 'linear'}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 8000, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 6400, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 4800, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 3200, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 1600, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 8000, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 6400, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 4800, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 3200, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1600, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 8000, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 6400, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 4800, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 3200, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1600, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.02, 'max_depth': 3, 'n_estimators': 8000, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.02, 'max_depth': 3, 'n_estimators': 6400, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.02, 'max_depth': 3, 'n_estimators': 4800, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.02, 'max_depth': 3, 'n_estimators': 3200, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.02, 'max_depth': 3, 'n_estimators': 1600, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 8000, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 6400, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 4800, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 3200, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 1600, 'subsample': 0.9}\n",
      "====================================================================================================\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1600}\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 3200}\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 4800}\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 6400}\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 8000}\n",
      "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 1600}\n",
      "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 6400}\n",
      "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 4800}\n",
      "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 3200}\n",
      "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 8000}\n",
      "{'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 1600}\n",
      "{'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 3200}\n",
      "{'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 4800}\n",
      "{'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 6400}\n",
      "{'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 8000}\n",
      "{'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 4800}\n",
      "{'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 3200}\n",
      "{'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 6400}\n",
      "{'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 8000}\n",
      "{'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 1600}\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 8000}\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 6400}\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 4800}\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 3200}\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 1600}\n",
      "{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 1600}\n",
      "{'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 1600}\n",
      "{'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 1600}\n",
      "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 1600}\n",
      "{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 640}\n",
      "{'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 640}\n",
      "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 640}\n",
      "{'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 640}\n",
      "{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 1250}\n",
      "{'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 1250}\n",
      "{'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 1250}\n",
      "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 1250}\n",
      "{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "{'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "{'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 2000}\n",
      "{'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 2000}\n",
      "{'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 2000}\n",
      "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 2000}\n",
      "{'max_depth': 8, 'min_samples_split': 3, 'n_estimators': 1600}\n",
      "{'max_depth': 12, 'min_samples_split': 3, 'n_estimators': 1600}\n",
      "{'max_depth': 10, 'min_samples_split': 3, 'n_estimators': 1600}\n",
      "{'max_depth': 15, 'min_samples_split': 3, 'n_estimators': 1600}\n",
      "{'max_depth': 10, 'min_samples_split': 3, 'n_estimators': 640}\n",
      "{'C': 1, 'epsilon': 2.5e-05, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 1.25e-05, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 5e-05, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 0.0001, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 0.0002, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 0.0004, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 0.0008, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 0.0016, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 0.0032, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 0.0063, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 0.0125, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 0.025, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 0.05, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 0.1, 'kernel': 'linear'}\n",
      "{'C': 1.5, 'epsilon': 1.25e-05, 'kernel': 'linear'}\n",
      "{'C': 1.5, 'epsilon': 2.5e-05, 'kernel': 'linear'}\n",
      "{'C': 1.5, 'epsilon': 5e-05, 'kernel': 'linear'}\n",
      "{'C': 1.5, 'epsilon': 0.0001, 'kernel': 'linear'}\n",
      "{'C': 1.5, 'epsilon': 0.0002, 'kernel': 'linear'}\n",
      "{'C': 1.5, 'epsilon': 0.0004, 'kernel': 'linear'}\n",
      "{'C': 1.5, 'epsilon': 0.0008, 'kernel': 'linear'}\n",
      "{'C': 1.5, 'epsilon': 0.0016, 'kernel': 'linear'}\n",
      "{'C': 1.5, 'epsilon': 0.0032, 'kernel': 'linear'}\n",
      "{'C': 1.5, 'epsilon': 0.0063, 'kernel': 'linear'}\n",
      "{'C': 1.5, 'epsilon': 0.0125, 'kernel': 'linear'}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 8000, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 6400, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 3200, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 4800, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 1600, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 1600, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 8000, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 6400, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 3200, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 4800, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 1600, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 6400, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 4800, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 3200, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 8000, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 1600, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 3200, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 4800, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 8000, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 6400, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 4800, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 8000, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 3200, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 6400, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 1600, 'subsample': 0.5}\n"
     ]
    }
   ],
   "source": [
    "top_summary_q = combine_and_top_logs(log_dir=\"model_dl_logs_q\", tops=25)\n",
    "for d in top_summary_q['model_hyperparameters_dict']:\n",
    "    print(json.loads(d))\n",
    "    \n",
    "print(\"\\n\"+\"=\"*100+\"\\n\")\n",
    "\n",
    "top_summary_d = combine_and_top_logs(log_dir=\"model_dl_logs_d\", tops=25)\n",
    "for d in top_summary_d['model_hyperparameters_dict']:\n",
    "    print(json.loads(d))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
