{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For metrics and splitting\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout=0.0):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        # final output layer\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, input_dim)\n",
    "        return self.net(x)  # shape: (batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout=0.0):\n",
    "        super(RNN, self).__init__()\n",
    "        # We'll use simple RNN\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=n_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if n_layers>1 else 0.0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len=1, input_dim)\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim, device=x.device)\n",
    "\n",
    "        out, _ = self.rnn(x, h0) \n",
    "        # out shape: (batch_size, seq_len, hidden_dim)\n",
    "        out = out[:, -1, :]   # take last time step\n",
    "        out = self.fc(out)    # shape: (batch_size, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout=0.0):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=n_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if n_layers>1 else 0.0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len=1, input_dim)\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim, device=x.device)\n",
    "        c0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim, device=x.device)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        # out shape: (batch_size, seq_len, hidden_dim)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout=0.0):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=n_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if n_layers>1 else 0.0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim, device=x.device)\n",
    "\n",
    "        out, hn = self.gru(x, h0)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dl_model(model_name, input_dim, params):\n",
    "    \"\"\"\n",
    "    Builds a PyTorch model instance given model_name and hyperparams.\n",
    "    model_name: 'MLP', 'RNN', 'LSTM', 'GRU'\n",
    "    params:\n",
    "      - 'hidden_dim': dimension of hidden layer(s)\n",
    "      - 'n_layers': number of layers (2,3,4)\n",
    "      - 'dropout': dropout rate\n",
    "      ...\n",
    "    \"\"\"\n",
    "    hidden_dim = params.get(\"hidden_dim\", 64)\n",
    "    n_layers   = params.get(\"n_layers\", 2)\n",
    "    dropout    = params.get(\"dropout\", 0.0)\n",
    "    \n",
    "    if model_name == \"MLP\":\n",
    "        model = MLP(input_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "    elif model_name == \"RNN\":\n",
    "        model = RNN(input_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "    elif model_name == \"LSTM\":\n",
    "        model = LSTM(input_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "    elif model_name == \"GRU\":\n",
    "        model = GRU(input_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_name}\")\n",
    "    \n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_forecast_with_torch(\n",
    "    df,\n",
    "    target_col,\n",
    "    model_name,\n",
    "    model_params,\n",
    "    window_size=25,\n",
    "    test_ratio=0.2,\n",
    "    drop_cols=None,\n",
    "    log_dir=\"model_dl_logs\",\n",
    "    log_filename=\"prediction_log.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Rolls a window of size window_size through the data.\n",
    "    For each window:\n",
    "      - split train/test by test_ratio\n",
    "      - standard scale X and y\n",
    "      - train a PyTorch model\n",
    "      - predict and inverse scale\n",
    "      - log metrics (MSE, MAPE, RÂ²)\n",
    "    \"\"\"\n",
    "    if drop_cols is None:\n",
    "        drop_cols = []\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    log_path = os.path.join(log_dir, log_filename)\n",
    "    log_entries = []\n",
    "\n",
    "    df = df.dropna().sort_index()\n",
    "    n = len(df)\n",
    "    \n",
    "    for start in range(0, n - window_size + 1):\n",
    "        window_df = df.iloc[start:start + window_size].copy()\n",
    "        X_window = window_df.drop(columns=[target_col] + drop_cols, errors='ignore')\n",
    "        y_window = window_df[target_col].values\n",
    "\n",
    "        train_size = int(len(X_window) * (1 - test_ratio))\n",
    "        if train_size < 1 or train_size >= len(X_window):\n",
    "            continue\n",
    "        \n",
    "        X_train = X_window.iloc[:train_size].values\n",
    "        X_test  = X_window.iloc[train_size:].values\n",
    "        y_train = y_window[:train_size]\n",
    "        y_test  = y_window[train_size:]\n",
    "\n",
    "        # Scale features\n",
    "        scaler_X = StandardScaler()\n",
    "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "        X_test_scaled  = scaler_X.transform(X_test)\n",
    "\n",
    "        # Scale target\n",
    "        scaler_y = StandardScaler()\n",
    "        y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "        y_test_scaled  = scaler_y.transform(y_test.reshape(-1, 1)).ravel()\n",
    "\n",
    "        # Format for model\n",
    "        input_dim = X_train_scaled.shape[1]\n",
    "        if model_name in [\"RNN\", \"LSTM\", \"GRU\"]:\n",
    "            X_train_scaled = np.expand_dims(X_train_scaled, axis=1)\n",
    "            X_test_scaled = np.expand_dims(X_test_scaled, axis=1)\n",
    "\n",
    "        X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "        y_train_t = torch.tensor(y_train_scaled, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        X_test_t = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "        y_test_t = torch.tensor(y_test_scaled, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "        # Build and train model\n",
    "        model = build_dl_model(model_name, input_dim, model_params)\n",
    "        lr = model_params.get(\"learning_rate\", 1e-3)\n",
    "        optimizer = model_params.get(\"optimizer\", \"adam\")\n",
    "        epochs = model_params.get(\"epochs\", 20)\n",
    "        batch_size = model_params.get(\"batch_size\", 32)\n",
    "\n",
    "        if optimizer.lower() == \"adam\":\n",
    "            opt = optim.Adam(model.parameters(), lr=lr)\n",
    "        else:\n",
    "            opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        dataset_size = X_train_t.shape[0]\n",
    "        num_batches = (dataset_size + batch_size - 1) // batch_size\n",
    "        model.train()\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            perm = torch.randperm(dataset_size)\n",
    "            X_train_t = X_train_t[perm]\n",
    "            y_train_t = y_train_t[perm]\n",
    "\n",
    "            for b_idx in range(num_batches):\n",
    "                start_b = b_idx * batch_size\n",
    "                end_b = min(start_b + batch_size, dataset_size)\n",
    "                xb = X_train_t[start_b:end_b]\n",
    "                yb = y_train_t[start_b:end_b]\n",
    "\n",
    "                opt.zero_grad()\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "        # Inference\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds_test = model(X_test_t)\n",
    "        preds_test = preds_test.cpu().numpy().ravel()\n",
    "        y_true_test = y_test_t.cpu().numpy().ravel()\n",
    "\n",
    "        # Inverse transform\n",
    "        preds_test = scaler_y.inverse_transform(preds_test.reshape(-1, 1)).ravel()\n",
    "        y_true_test = scaler_y.inverse_transform(y_true_test.reshape(-1, 1)).ravel()\n",
    "\n",
    "        # Guard against invalid output\n",
    "        if np.any(np.isnan(preds_test)) or np.any(np.isinf(preds_test)):\n",
    "            print(\"â ï¸ Skipping window due to NaN/inf in predictions.\")\n",
    "            continue\n",
    "\n",
    "        mse = mean_squared_error(y_true_test, preds_test)\n",
    "        mape = mean_absolute_percentage_error(y_true_test, preds_test)\n",
    "        r2 = r2_score(y_true_test, preds_test)\n",
    "\n",
    "        log_entry = {\n",
    "            \"model_name\": model_name,\n",
    "            \"model_hyperparameters_dict\": json.dumps(model_params),\n",
    "            \"window_size\": window_size,\n",
    "            \"test_ratio\": test_ratio,\n",
    "            \"start_date\": str(df.index[start]),\n",
    "            \"end_date\": str(df.index[start + window_size - 1]),\n",
    "            \"test_data_values_list\": y_true_test.tolist(),\n",
    "            \"test_data_model_predictions_list\": preds_test.tolist(),\n",
    "            \"MSE_score\": mse,\n",
    "            \"MAPE_score\": mape,\n",
    "            \"R^2_score\": r2\n",
    "        }\n",
    "        log_entries.append(log_entry)\n",
    "\n",
    "    log_df = pd.DataFrame(log_entries)\n",
    "    if os.path.exists(log_path):\n",
    "        existing = pd.read_csv(log_path)\n",
    "        log_df = pd.concat([existing, log_df], ignore_index=True)\n",
    "    log_df.to_csv(log_path, index=False)\n",
    "    return log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_torch_model(\n",
    "    df,\n",
    "    target_col,\n",
    "    model_type,\n",
    "    param_grid,\n",
    "    window_sizes=[25, 50],\n",
    "    test_ratio=0.2,\n",
    "    drop_cols=None,\n",
    "    log_dir=\"model_dl_logs\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Similar approach as classical ML tune_model. \n",
    "    We'll iterate over param_grid, run sliding_window_forecast_with_torch, \n",
    "    log results, average, and return summary.\n",
    "    \"\"\"\n",
    "    if drop_cols is None:\n",
    "        drop_cols = []\n",
    "    \n",
    "    param_list = list(ParameterGrid(param_grid))\n",
    "    total_iterations = len(param_list) * len(window_sizes)\n",
    "    pbar = tqdm(total=total_iterations, desc=f\"Tuning {model_type}\")\n",
    "    \n",
    "    summaries = []\n",
    "    for w in window_sizes:\n",
    "        for params in param_list:\n",
    "            log_df = sliding_window_forecast_with_torch(\n",
    "                df=df,\n",
    "                target_col=target_col,\n",
    "                model_name=model_type,\n",
    "                model_params=params,\n",
    "                window_size=w,\n",
    "                test_ratio=test_ratio,\n",
    "                drop_cols=drop_cols,\n",
    "                log_dir=log_dir,\n",
    "                log_filename=f\"{model_type}_window_{w}.csv\"\n",
    "            )\n",
    "            \n",
    "            pbar.update(1)\n",
    "            if log_df.empty:\n",
    "                continue\n",
    "\n",
    "            avg_mse  = log_df[\"MSE_score\"].mean()\n",
    "            avg_mape = log_df[\"MAPE_score\"].mean()\n",
    "            avg_r2   = log_df[\"R^2_score\"].mean()\n",
    "\n",
    "            summary_entry = {\n",
    "                \"model_name\": model_type,\n",
    "                \"model_hyperparameters_dict\": json.dumps(params),\n",
    "                \"window_size\": w,\n",
    "                \"test_ratio\": test_ratio,\n",
    "                \"avg_MSE\": avg_mse,\n",
    "                \"avg_MAPE\": avg_mape,\n",
    "                \"avg_R^2\": avg_r2\n",
    "            }\n",
    "            summaries.append(summary_entry)\n",
    "    pbar.close()\n",
    "    \n",
    "    if summaries:\n",
    "        return pd.DataFrame(summaries)\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_top_logs(log_dir=\"model_dl_logs\", tops=5):\n",
    "    \"\"\"\n",
    "    Reads all CSV files in the given log directory, \n",
    "    groups by model_name and hyperparameters, \n",
    "    picks top \"tops\" by MAPE ascending for each model.\n",
    "    \"\"\"\n",
    "    all_files = [f for f in os.listdir(log_dir) if f.endswith(\".csv\")]\n",
    "    if not all_files:\n",
    "        print(f\"No CSV log files in {log_dir} directory.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    dfs = [pd.read_csv(os.path.join(log_dir, f)) for f in all_files]\n",
    "    combined_logs = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    grouped_summary = combined_logs.groupby(\n",
    "        [\"model_name\",\"model_hyperparameters_dict\"]\n",
    "    )[[\"MSE_score\",\"MAPE_score\",\"R^2_score\"]].mean().reset_index()\n",
    "\n",
    "    top_n_list = []\n",
    "    for model in grouped_summary[\"model_name\"].unique():\n",
    "        sub = grouped_summary[grouped_summary[\"model_name\"] == model]\n",
    "        top_n = sub.sort_values(\"MAPE_score\", ascending=True).head(tops)\n",
    "        top_n_list.append(top_n)\n",
    "    top_n_combined = pd.concat(top_n_list, ignore_index=True)\n",
    "    return top_n_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids_torch = {\n",
    "    \"MLP\": {\n",
    "        \"hidden_dim\":    [32, 64, 128],\n",
    "        \"n_layers\":      [1, 2, 3],\n",
    "        \"dropout\":       [0.0, 0.2, 0.4],                # Regularization\n",
    "        \"learning_rate\": [1e-2, 1e-3, 1e-4],             # Wider range\n",
    "        \"optimizer\":     [\"adam\"],\n",
    "        \"epochs\":        [25, 50, 100],                  # Reasonable ranges\n",
    "        \"batch_size\":    [16, 32, 64],\n",
    "        \"weight_decay\":  [0.0, 1e-4, 1e-3],              # L2 regularization\n",
    "    },\n",
    "\n",
    "    \"RNN\": {\n",
    "        \"hidden_dim\":    [32, 64, 128],\n",
    "        \"n_layers\":      [1, 2, 3],\n",
    "        \"dropout\":       [0.0, 0.2, 0.4], \n",
    "        \"learning_rate\": [1e-2, 1e-3, 1e-4],\n",
    "        \"optimizer\":     [\"adam\"],\n",
    "        \"epochs\":        [25, 50, 100],\n",
    "        \"batch_size\":    [16, 32, 64],\n",
    "        \"weight_decay\":  [0.0, 1e-4, 1e-3]\n",
    "    },\n",
    "\n",
    "    \"LSTM\": {\n",
    "        \"hidden_dim\":    [64, 128, 256],\n",
    "        \"n_layers\":      [1, 2, 3],\n",
    "        \"dropout\":       [0.0, 0.2, 0.4], \n",
    "        \"learning_rate\": [1e-2, 1e-3, 1e-4],\n",
    "        \"optimizer\":     [\"adam\"],\n",
    "        \"epochs\":        [25, 50, 100],\n",
    "        \"batch_size\":    [16, 32, 64],\n",
    "        \"weight_decay\":  [0.0, 1e-4, 1e-3]\n",
    "    },\n",
    "\n",
    "    \"GRU\": {\n",
    "        \"hidden_dim\":    [64, 128, 256],\n",
    "        \"n_layers\":      [1, 2, 3],\n",
    "        \"dropout\":       [0.0, 0.2, 0.4], \n",
    "        \"learning_rate\": [1e-2, 1e-3, 1e-4],\n",
    "        \"optimizer\":     [\"adam\"],\n",
    "        \"epochs\":        [25, 50, 100],\n",
    "        \"batch_size\":    [16, 32, 64],\n",
    "        \"weight_decay\":  [0.0, 1e-4, 1e-3]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['AAPL', 'MSFT', 'LLY', 'UNH', 'V', 'MA', 'GOOGL', 'META', 'AMZN', 'TSLA', 'PG', 'WMT', 'RTX', 'UNP', 'XOM', 'CVX', 'LIN', 'SHW', 'AMT', 'PLD', 'NEE', 'SO'])\n"
     ]
    }
   ],
   "source": [
    "with open(\"temp_output/quarterly_X_y.pkl\", \"rb\") as f:\n",
    "    data_q = pickle.load(f)\n",
    "\n",
    "drop_columns = []\n",
    "print(data_q.keys())  # Should show all the tickers like AAPL, MSFT, etc.\n",
    "quaterly_data = data_q[\"AAPL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Tuning RNN:   0%|          | 8/2187 [00:59<4:29:30,  7.42s/it]\n",
      "Tuning MLP:   4%|â         | 83/2187 [02:29<1:03:00,  1.80s/it]\n",
      "Tuning RNN:   0%|          | 9/2187 [00:55<3:41:50,  6.11s/it]\n",
      "Tuning RNN:   1%|          | 11/2187 [00:48<2:39:34,  4.40s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# RNN on quarterly\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m rnn_summary_q = \u001b[43mtune_torch_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquaterly_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43my\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRNN\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparam_grids_torch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRNN\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwindow_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_dl_logs_q\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# LSTM on quarterly\u001b[39;00m\n\u001b[32m     14\u001b[39m lstm_summary_q = tune_torch_model(\n\u001b[32m     15\u001b[39m     df=quaterly_data,\n\u001b[32m     16\u001b[39m     target_col=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     log_dir=\u001b[33m\"\u001b[39m\u001b[33mmodel_dl_logs_q\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mtune_torch_model\u001b[39m\u001b[34m(df, target_col, model_type, param_grid, window_sizes, test_ratio, drop_cols, log_dir)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m window_sizes:\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m param_list:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m         log_df = \u001b[43msliding_window_forecast_with_torch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtest_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdrop_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlog_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_window_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mw\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m         pbar.update(\u001b[32m1\u001b[39m)\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m log_df.empty:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 118\u001b[39m, in \u001b[36msliding_window_forecast_with_torch\u001b[39m\u001b[34m(df, target_col, model_name, model_params, window_size, test_ratio, drop_cols, log_dir, log_filename)\u001b[39m\n\u001b[32m    116\u001b[39m mse = mean_squared_error(y_true_test, preds_test)\n\u001b[32m    117\u001b[39m mape = mean_absolute_percentage_error(y_true_test, preds_test)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m r2 = \u001b[43mr2_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m log_entry = {\n\u001b[32m    121\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m\"\u001b[39m: model_name,\n\u001b[32m    122\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel_hyperparameters_dict\u001b[39m\u001b[33m\"\u001b[39m: json.dumps(model_params),\n\u001b[32m   (...)\u001b[39m\u001b[32m    131\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mR^2_score\u001b[39m\u001b[33m\"\u001b[39m: r2\n\u001b[32m    132\u001b[39m }\n\u001b[32m    133\u001b[39m log_entries.append(log_entry)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Spring 2025/STAT5398/XAI/Code/Pipeline/xaivenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Spring 2025/STAT5398/XAI/Code/Pipeline/xaivenv/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1277\u001b[39m, in \u001b[36mr2_score\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput, force_finite)\u001b[39m\n\u001b[32m   1273\u001b[39m     weight = \u001b[32m1.0\u001b[39m\n\u001b[32m   1275\u001b[39m numerator = xp.sum(weight * (y_true - y_pred) ** \u001b[32m2\u001b[39m, axis=\u001b[32m0\u001b[39m)\n\u001b[32m   1276\u001b[39m denominator = xp.sum(\n\u001b[32m-> \u001b[39m\u001b[32m1277\u001b[39m     weight * (y_true - \u001b[43m_average\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m) ** \u001b[32m2\u001b[39m,\n\u001b[32m   1278\u001b[39m     axis=\u001b[32m0\u001b[39m,\n\u001b[32m   1279\u001b[39m )\n\u001b[32m   1281\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _assemble_r2_explained_variance(\n\u001b[32m   1282\u001b[39m     numerator=numerator,\n\u001b[32m   1283\u001b[39m     denominator=denominator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1288\u001b[39m     device=device_,\n\u001b[32m   1289\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Spring 2025/STAT5398/XAI/Code/Pipeline/xaivenv/lib/python3.11/site-packages/sklearn/utils/_array_api.py:716\u001b[39m, in \u001b[36m_average\u001b[39m\u001b[34m(a, axis, weights, normalize, xp)\u001b[39m\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m    715\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray(\u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43maverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    718\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray(numpy.dot(a, weights))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Spring 2025/STAT5398/XAI/Code/Pipeline/xaivenv/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:553\u001b[39m, in \u001b[36maverage\u001b[39m\u001b[34m(a, axis, weights, returned, keepdims)\u001b[39m\n\u001b[32m    550\u001b[39m a = np.asanyarray(a)\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m     axis = \u001b[43m_nx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize_axis_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maxis\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m np._NoValue:\n\u001b[32m    556\u001b[39m     \u001b[38;5;66;03m# Don't pass on the keepdims argument if one wasn't given.\u001b[39;00m\n\u001b[32m    557\u001b[39m     keepdims_kw = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Spring 2025/STAT5398/XAI/Code/Pipeline/xaivenv/lib/python3.11/site-packages/numpy/_core/numeric.py:1439\u001b[39m, in \u001b[36mnormalize_axis_tuple\u001b[39m\u001b[34m(axis, ndim, argname, allow_duplicate)\u001b[39m\n\u001b[32m   1437\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1438\u001b[39m \u001b[38;5;66;03m# Going via an iterator directly is slower than via list comprehension.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1439\u001b[39m axis = \u001b[38;5;28mtuple\u001b[39m(\u001b[43m[\u001b[49m\u001b[43mnormalize_axis_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43max\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m   1440\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_duplicate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(axis)) != \u001b[38;5;28mlen\u001b[39m(axis):\n\u001b[32m   1441\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m argname:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Spring 2025/STAT5398/XAI/Code/Pipeline/xaivenv/lib/python3.11/site-packages/numpy/_core/numeric.py:1439\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1437\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1438\u001b[39m \u001b[38;5;66;03m# Going via an iterator directly is slower than via list comprehension.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1439\u001b[39m axis = \u001b[38;5;28mtuple\u001b[39m([normalize_axis_index(ax, ndim, argname) \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m axis])\n\u001b[32m   1440\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_duplicate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(axis)) != \u001b[38;5;28mlen\u001b[39m(axis):\n\u001b[32m   1441\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m argname:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# MLP on quarterly\n",
    "mlp_summary_q = tune_torch_model(\n",
    "    df=quaterly_data,\n",
    "    target_col=\"y\",\n",
    "    model_type=\"MLP\",\n",
    "    param_grid=param_grids_torch[\"MLP\"],\n",
    "    window_sizes=[10],  # or [25,50]\n",
    "    test_ratio=0.2,\n",
    "    drop_cols=drop_columns,\n",
    "    log_dir=\"model_dl_logs_q\"\n",
    ")\n",
    "\n",
    "# RNN on quarterly\n",
    "rnn_summary_q = tune_torch_model(\n",
    "    df=quaterly_data,\n",
    "    target_col=\"y\",\n",
    "    model_type=\"RNN\",\n",
    "    param_grid=param_grids_torch[\"RNN\"],\n",
    "    window_sizes=[10],\n",
    "    test_ratio=0.2,\n",
    "    drop_cols=drop_columns,\n",
    "    log_dir=\"model_dl_logs_q\"\n",
    ")\n",
    "\n",
    "# LSTM on quarterly\n",
    "lstm_summary_q = tune_torch_model(\n",
    "    df=quaterly_data,\n",
    "    target_col=\"y\",\n",
    "    model_type=\"LSTM\",\n",
    "    param_grid=param_grids_torch[\"LSTM\"],\n",
    "    window_sizes=[10],\n",
    "    test_ratio=0.2,\n",
    "    drop_cols=drop_columns,\n",
    "    log_dir=\"model_dl_logs_q\"\n",
    ")\n",
    "\n",
    "# GRU on quarterly\n",
    "gru_summary_q = tune_torch_model(\n",
    "    df=quaterly_data,\n",
    "    target_col=\"y\",\n",
    "    model_type=\"GRU\",\n",
    "    param_grid=param_grids_torch[\"GRU\"],\n",
    "    window_sizes=[10],\n",
    "    test_ratio=0.2,\n",
    "    drop_cols=drop_columns,\n",
    "    log_dir=\"model_dl_logs_q\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_summary_q = pd.concat([rf_summary_q, svm_summary_q, gb_summary_q, xgb_summary_q], ignore_index=True)\n",
    "print(\"Combined Tuning Summary:\")\n",
    "display(combined_summary_q)\n",
    "\n",
    "# Alternatively, read all prediction log files from log directory\n",
    "top5_summary_q = combine_and_top_logs(log_dir=\"model_dl_logs_q\")\n",
    "print(\"Top 5 Configurations per Model:\")\n",
    "display(top5_summary_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume', '^GSPC', '^IXIC',\n",
      "       '^DJI', '^VIX', 'CL=F', 'GC=F', 'SI=F', '^TNX', 'DX-Y.NYB',\n",
      "       'FedFundsRate', 'SMA5', 'SMA50', 'SMA200', 'MACDLine', 'MACDSignal',\n",
      "       'MACDHist', 'RSI14', 'BBupper', 'BBlower', 'ROC12', 'PPO', 'MOM5',\n",
      "       'StochK', 'StochD', 'WillR', 'AccDist', 'PlusDI14', 'TR', 'SlowStochD',\n",
      "       'ChaikinOsc', 'ADX14', 'ATR14', 'y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "with open(\"temp_output/daily_X_y.pkl\", \"rb\") as f:\n",
    "    data_d = pickle.load(f)\n",
    "    \n",
    "drop_columns = []\n",
    "print(data_d.keys())  # Should show all the tickers like AAPL, MSFT, etc.\n",
    "daily_data = data_d[\"AAPL\"]\n",
    "daily_data = daily_data.iloc[-128:,:]\n",
    "daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning RF: 100%|ââââââââââ| 48/48 [34:52<00:00, 43.59s/it]\n",
      "Tuning SVM: 100%|ââââââââââ| 798/798 [03:36<00:00,  3.68it/s]\n",
      "Tuning GB: 100%|ââââââââââ| 50/50 [51:04<00:00, 61.29s/it] \n",
      "Tuning XGB: 100%|ââââââââââ| 450/450 [58:51:49<00:00, 470.91s/it]   \n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "mlp_summary_d = tune_torch_model(\n",
    "    df=daily_data,\n",
    "    target_col=\"y\",\n",
    "    model_type=\"MLP\",\n",
    "    param_grid=param_grids_torch[\"MLP\"],\n",
    "    window_sizes=[25],\n",
    "    test_ratio=0.2,\n",
    "    drop_cols=drop_columns,\n",
    "    log_dir=\"model_dl_logs_d\"\n",
    ")\n",
    "\n",
    "# RNN\n",
    "rnn_summary_d = tune_torch_model(\n",
    "    df=daily_data,\n",
    "    target_col=\"y\",\n",
    "    model_type=\"RNN\",\n",
    "    param_grid=param_grids_torch[\"RNN\"],\n",
    "    window_sizes=[25],\n",
    "    test_ratio=0.2,\n",
    "    drop_cols=drop_columns,\n",
    "    log_dir=\"model_dl_logs_d\"\n",
    ")\n",
    "\n",
    "# LSTM\n",
    "lstm_summary_d = tune_torch_model(\n",
    "    df=daily_data,\n",
    "    target_col=\"y\",\n",
    "    model_type=\"LSTM\",\n",
    "    param_grid=param_grids_torch[\"LSTM\"],\n",
    "    window_sizes=[25],\n",
    "    test_ratio=0.2,\n",
    "    drop_cols=drop_columns,\n",
    "    log_dir=\"model_dl_logs_d\"\n",
    ")\n",
    "\n",
    "# GRU\n",
    "gru_summary_d = tune_torch_model(\n",
    "    df=daily_data,\n",
    "    target_col=\"y\",\n",
    "    model_type=\"GRU\",\n",
    "    param_grid=param_grids_torch[\"GRU\"],\n",
    "    window_sizes=[25],\n",
    "    test_ratio=0.2,\n",
    "    drop_cols=drop_columns,\n",
    "    log_dir=\"model_dl_logs_d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_summary_d = pd.concat([rf_summary_d, svm_summary_d, gb_summary_d, xgb_summary_d], ignore_index=True)\n",
    "print(\"Combined Tuning Summary:\")\n",
    "display(combined_summary_d)\n",
    "\n",
    "# Alternatively, read all prediction log files from log directory\n",
    "top5_summary_d = combine_and_top_logs(log_dir=\"model_dl_logs_d\")\n",
    "print(\"Top 5 Configurations per Model:\")\n",
    "display(top5_summary_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 8000}\n",
      "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 6400}\n",
      "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 4800}\n",
      "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 3200}\n",
      "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 1600}\n",
      "{'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 8000}\n",
      "{'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 6400}\n",
      "{'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 4800}\n",
      "{'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 3200}\n",
      "{'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 1600}\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 4800}\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 6400}\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 8000}\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 3200}\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1600}\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 1600}\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 4800}\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 6400}\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 8000}\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 3200}\n",
      "{'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 8000}\n",
      "{'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 6400}\n",
      "{'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 4800}\n",
      "{'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 3200}\n",
      "{'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 1600}\n",
      "{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 2000}\n",
      "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 2000}\n",
      "{'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 2000}\n",
      "{'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 2000}\n",
      "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 1600}\n",
      "{'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 1600}\n",
      "{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 1600}\n",
      "{'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 1600}\n",
      "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 1250}\n",
      "{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 1250}\n",
      "{'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 1250}\n",
      "{'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 1250}\n",
      "{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "{'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "{'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "{'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 800}\n",
      "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 800}\n",
      "{'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 800}\n",
      "{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 800}\n",
      "{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 640}\n",
      "{'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 640}\n",
      "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 640}\n",
      "{'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 640}\n",
      "{'max_depth': 12, 'min_samples_split': 3, 'n_estimators': 2000}\n",
      "{'C': 6, 'epsilon': 1.25e-05, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 2.5e-05, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 5e-05, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 0.0001, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 0.0002, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 0.0004, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 0.0008, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 0.0016, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 0.0032, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 0.0063, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 0.0125, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 0.025, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 0.05, 'kernel': 'linear'}\n",
      "{'C': 6, 'epsilon': 0.1, 'kernel': 'linear'}\n",
      "{'C': 5.5, 'epsilon': 1.25e-05, 'kernel': 'linear'}\n",
      "{'C': 5.5, 'epsilon': 2.5e-05, 'kernel': 'linear'}\n",
      "{'C': 5.5, 'epsilon': 5e-05, 'kernel': 'linear'}\n",
      "{'C': 5.5, 'epsilon': 0.0001, 'kernel': 'linear'}\n",
      "{'C': 5.5, 'epsilon': 0.0002, 'kernel': 'linear'}\n",
      "{'C': 5.5, 'epsilon': 0.0004, 'kernel': 'linear'}\n",
      "{'C': 5.5, 'epsilon': 0.0008, 'kernel': 'linear'}\n",
      "{'C': 5.5, 'epsilon': 0.0016, 'kernel': 'linear'}\n",
      "{'C': 5.5, 'epsilon': 0.0032, 'kernel': 'linear'}\n",
      "{'C': 5.5, 'epsilon': 0.0063, 'kernel': 'linear'}\n",
      "{'C': 5.5, 'epsilon': 0.0125, 'kernel': 'linear'}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 8000, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 6400, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 4800, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 3200, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 1600, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 8000, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 6400, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 4800, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 3200, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1600, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 8000, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 6400, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 4800, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 3200, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1600, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.02, 'max_depth': 3, 'n_estimators': 8000, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.02, 'max_depth': 3, 'n_estimators': 6400, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.02, 'max_depth': 3, 'n_estimators': 4800, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.02, 'max_depth': 3, 'n_estimators': 3200, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.02, 'max_depth': 3, 'n_estimators': 1600, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 8000, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 6400, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 4800, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 3200, 'subsample': 0.9}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 1600, 'subsample': 0.9}\n",
      "====================================================================================================\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1600}\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 3200}\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 4800}\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 6400}\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 8000}\n",
      "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 1600}\n",
      "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 6400}\n",
      "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 4800}\n",
      "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 3200}\n",
      "{'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 8000}\n",
      "{'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 1600}\n",
      "{'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 3200}\n",
      "{'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 4800}\n",
      "{'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 6400}\n",
      "{'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 8000}\n",
      "{'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 4800}\n",
      "{'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 3200}\n",
      "{'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 6400}\n",
      "{'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 8000}\n",
      "{'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 1600}\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 8000}\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 6400}\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 4800}\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 3200}\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 1600}\n",
      "{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 1600}\n",
      "{'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 1600}\n",
      "{'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 1600}\n",
      "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 1600}\n",
      "{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 640}\n",
      "{'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 640}\n",
      "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 640}\n",
      "{'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 640}\n",
      "{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 1250}\n",
      "{'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 1250}\n",
      "{'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 1250}\n",
      "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 1250}\n",
      "{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "{'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "{'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 2000}\n",
      "{'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 2000}\n",
      "{'max_depth': 12, 'min_samples_split': 2, 'n_estimators': 2000}\n",
      "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 2000}\n",
      "{'max_depth': 8, 'min_samples_split': 3, 'n_estimators': 1600}\n",
      "{'max_depth': 12, 'min_samples_split': 3, 'n_estimators': 1600}\n",
      "{'max_depth': 10, 'min_samples_split': 3, 'n_estimators': 1600}\n",
      "{'max_depth': 15, 'min_samples_split': 3, 'n_estimators': 1600}\n",
      "{'max_depth': 10, 'min_samples_split': 3, 'n_estimators': 640}\n",
      "{'C': 1, 'epsilon': 2.5e-05, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 1.25e-05, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 5e-05, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 0.0001, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 0.0002, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 0.0004, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 0.0008, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 0.0016, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 0.0032, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 0.0063, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 0.0125, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 0.025, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 0.05, 'kernel': 'linear'}\n",
      "{'C': 1, 'epsilon': 0.1, 'kernel': 'linear'}\n",
      "{'C': 1.5, 'epsilon': 1.25e-05, 'kernel': 'linear'}\n",
      "{'C': 1.5, 'epsilon': 2.5e-05, 'kernel': 'linear'}\n",
      "{'C': 1.5, 'epsilon': 5e-05, 'kernel': 'linear'}\n",
      "{'C': 1.5, 'epsilon': 0.0001, 'kernel': 'linear'}\n",
      "{'C': 1.5, 'epsilon': 0.0002, 'kernel': 'linear'}\n",
      "{'C': 1.5, 'epsilon': 0.0004, 'kernel': 'linear'}\n",
      "{'C': 1.5, 'epsilon': 0.0008, 'kernel': 'linear'}\n",
      "{'C': 1.5, 'epsilon': 0.0016, 'kernel': 'linear'}\n",
      "{'C': 1.5, 'epsilon': 0.0032, 'kernel': 'linear'}\n",
      "{'C': 1.5, 'epsilon': 0.0063, 'kernel': 'linear'}\n",
      "{'C': 1.5, 'epsilon': 0.0125, 'kernel': 'linear'}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 8000, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 6400, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 3200, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 4800, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 2, 'n_estimators': 1600, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 1600, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 8000, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 6400, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 3200, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 4800, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 1600, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 6400, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 4800, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 3200, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 8000, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 1600, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 3200, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 4800, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 8000, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.02, 'max_depth': 2, 'n_estimators': 6400, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 4800, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 8000, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 3200, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 6400, 'subsample': 0.5}\n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.04, 'max_depth': 3, 'n_estimators': 1600, 'subsample': 0.5}\n"
     ]
    }
   ],
   "source": [
    "top_summary_q = combine_and_top_logs(log_dir=\"model_dl_logs_q\", tops=25)\n",
    "for d in top_summary_q['model_hyperparameters_dict']:\n",
    "    print(json.loads(d))\n",
    "    \n",
    "print(\"\\n\"+\"=\"*100+\"\\n\")\n",
    "\n",
    "top_summary_d = combine_and_top_logs(log_dir=\"model_dl_logs_d\", tops=25)\n",
    "for d in top_summary_d['model_hyperparameters_dict']:\n",
    "    print(json.loads(d))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xaivenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
